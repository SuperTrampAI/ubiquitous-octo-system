---
title: 入坑爬虫
categories: Coder
date: 2019-06-01 21:31:59
tags: 爬虫
description:
---

游走在网络，无声的幽灵

<!-- more -->

# 网络爬虫的定义：
1.类比：想象自己是一只蜘蛛，在互联网上爬，从一个点开始（随意的一个点）不断的从这一点去拓展，这个链接地址链接到了下一个链接地址，循环往复。(这个过程中有数据的判重问题)
2.把你所需要的“数据”从互联网上“背”回来，包括但不限于：出行，社交，电商，O2O，政府部门等数据
3.按照一定的规则，自动的抓取万维网信息的程序或者脚本。或者还有其他称呼：蚂蚁，自动索引。通俗的将，爬虫就是能够自动访问互联网并将网站内容下载下来的程序或脚本，类似一个机器人，能把别人网站的信息弄到自己电脑上，经过过滤，筛选，归纳，整理，排序，方便利用。

# 为什么需要网络爬虫：
获得自己的想要的数据：比如黄牛希望拿到12306票务网站的余票信息，出售给别人。比如在招聘网站上面，筛选出合适的公司，发出简历；比如在各大网站上面，找到自己需要的资料，论文信息。

# 网络爬虫的应用：
不单单的只是爬些数据！这是个有趣的问题（开脑冻的回答）
[你做过哪些有趣的基于网络爬虫的应用？](https://www.zhihu.com/question/22382568)

# 根据爬虫的定义，使用爬虫可以分为如下步骤：
1. 获取数据
http抓取工具：scrapy：由python编写的开源网络爬虫框架，用来爬去网络数据，提取结构性数据的框架。
Scrapy官网： https://scrapy.org/
在git bash here中使用命令：pip install scrapy 安装scrapy（需要先安装python（ http://python.org/download/）

除了使用pip安装scrapy以外，还可以使用Anaconda安装（ https://www.anaconda.com/distribution/））
在命令行使用pip --version 查看是否安装成功。 需要安装python库：pip install  requests

2. 数据过滤
Bloom Filter（解决判重问题：采用特别的算法，可能误删以及不删带来的数据不准确性）例子： https://llimllib.github.io/bloomfilter-tutorial/

3. 利用分布式提高效率
怎样维护一个所有集群能够有效分享的分布式队列（python-rq）： https://github.com/rq/rq
[Scrapy+redis](https://github.com/rmax/scrapy-redis )
4. 处理数据
[python-goose](https://github.com/grangier/python-goose)

简化操作：
	1. requests（入门）
	2. lxml（入门）
这两个库均为基本库，就算入门。

相关教程：[极客学院教程](
http://wiki.jikexueyuan.com/project/python-crawler-guide/the-use-of-urllib-library.html)
